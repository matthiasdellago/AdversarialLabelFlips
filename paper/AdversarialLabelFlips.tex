% !TeX spellcheck = en_GB
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{pgffor}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{amsmath,amssymb,amsfonts,amsthm,mathtools}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{diagbox}
\usepackage{float, multirow}
\usepackage{tikz, pgfplots}
\usepackage{tikzsymbols}
\usetikzlibrary{spy}
\usepackage{subcaption}
\usepgfplotslibrary{groupplots}
\pgfplotsset{compat=newest}
\usepackage{blindtext}


\newcommand{\defneq}{\mathrel{\mathop:}=}
\newcommand{\eqdefn}{=\mathrel{\mathop:}}

\begin{document}

\begin{titlepage}
	\noindent\makebox[.5\textwidth][l]{\includegraphics{universitaet-innsbruck-logo-cmyk-farbe.pdf}}
	\vspace{1cm}
	\begin{center}
		\includegraphics[width=.7\textwidth]{titlefigure.pdf}
		\vspace{50pt}\\
		\textbf{\Huge Adversarial Label Flips}
		\vspace{40pt}\\
		\textbf{\Large Matthias Dellago \& Maximilian Samsinger}\vspace{20pt}\\
		{\large\today}
	\end{center}
\end{titlepage}

	\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
	\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
	\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
	\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\begin{abstract}
	Given a neural network classifier and an untargeted evasion attack, in what class does the adversarial example fall post-attack?
	In the following, we will answer this question by evaluating some state of the art attacks on simple neural network classifiers trained on industry standard datasets.
	We discover that semantically similar classes are more likely to be confused with another, leading us to hypothesise that the convolutional neural networks recognise these similarities.
\end{abstract}

\section{Introduction}
In 2013 Szegedy et al. demonstrated that deep neural networks are susceptible to attacks \cite{Szegedy13}. These adversarial examples consist of a small perturbation applied to otherwise benign inputs, engineered to cause the neural network to misbehave.

We consider neural image classifiers, in particular convolutional neural networks \cite{lecun1999object} (CNN).
That is, inputs are images and attacks apply small changes to said images, designed to cause the CNN to misclassify the target.

These attacks on classifiers come in two different variations: targeted and untargeted.
Targeted attacks aim to cause a misclassification into a specific target class. For example, a cat image is to be misclassified as a dog.
Untargeted attacks only try to evade correct classification. In this case, the cat image must only be misclassified as anything other than a cat, not a dog specifically.
For a proper introduction to threat modelling in adversarial machine learning we refer to \cite{biggio2018wild}.

When considering untargeted attacks, the question what the adversarial image is classified as post-attack arises. This is what we will experimentally answer in this paper.

\begin{figure}[h]
	\begin{tabular}{cc|c|c|c|}
		& \multicolumn{1}{c}{} & \multicolumn{3}{c}{are categorised as}\\
		& \multicolumn{1}{c}{} & \multicolumn{1}{c}{Dog}  & \multicolumn{1}{c}{Cat} & \multicolumn{1}{c}{Plane} \\\cline{3-5}
		\multirow{3}*{Adversarial examples of a}  & Dog & 2 & 6 & 2\\\cline{3-5}
		& Cat & 7 & 3 &  0 \\\cline{3-5}
		& Plane & 1 & 2 &  7 \\\cline{3-5}
	\end{tabular}

	\caption{An example of a confusion Matrix. Unsuccessful attacks (along the diagonal) are also included.}
	\label{fig:matrixexample}
\end{figure}
We will present our results in terms of confusion matrices. An simple example is presented in Figure \ref{fig:matrixexample}. For large matrices numbers become more difficult to grasp, so we will display our results in heatmap-style images, as seen on the title page.

In our experiments (Section \ref{sec:experiments}) we used the Foolbox framework \cite{rauber2017foolbox}, and simple CNNs trained on the MNIST~\cite{deng2012mnist}, Fashion-MNIST~\cite{deng2012mnist} and CIFAR-10~\cite{krizhevsky2009learning} datasets. We applied three state of the art attacks: Projected Gradient Decent (PDG)\cite{madry2017towards}, Carlini-Wagner~\cite{carlini2017towards} and Brendel-Bethge~\cite{brendel2019accurate}.
We show that for the CIFAR-10 dataset the confusion matrices are surprisingly symmetric, and intuitively similar classes are often confused with each other. Furthermore we observe that for attacks which are allotted a large perturbation budget, there exist certain attractor classes, which most of the adversarial images are classified as. (Section \ref{sec:results})

\section{Background and related work}

\paragraph{Existence of adversarial examples}
Since adversarial examples were first introduced in \cite{Szegedy13} a large body of literature has flourished on the topic. In the following we will briefly introduce the attacks we use.

\subsection{Attacks}\label{sec:Attacks}

\paragraph{Fast gradient sign method}
Goodfellow et al. developed the fast gradient sign method (FGSM) \cite{goodfellow2014explaining}, making attacks fast and easy. Its key insight was that the backpropagation commonly used to update the weights and biases can be applied all the way back to the input data itself to yields the gradient of the cost function. They then apply gradient decent to find an adversarial example. Since they optimise for the $L^\infty$-norm, all entries of the perturbation are scaled to the same magnitude.

\paragraph{Projected gradient descent}
Projected gradient descent (PGD) was first shown in \cite{madry2017towards}. Conceptually, it is very similar to iterating FGSM until converging in a local misclassification optimum. The "projected" part of the name derives from the fact that upon leaving a ball of radius $\epsilon$, instead of continuing iteration, they project back onto said ball. Then iterated FGSM resumes.
This attack leads to formidable results, especially using the $L^\infty$-norm.

\paragraph{Carlini-Wagner attack}
Carlini and Wagner \cite{carlini2017towards} invented a different style of attack, where the cost function of the classifier and the distance of the adversarial example to the original are wrapped into one function. They can then simultaneously optimize for both using the Adam stochastic optimizer \cite{kingma2017adam}.

\paragraph{Brendel-Bethge attack}
Brendel and Bethe invented a quite different approach~\cite{brendel2019accurate}.
Their method works by starting from a image deep inside the misclassification region and then preforming binary search between it and the original image, to find the decision boundary. Once there, they move along the boundary to minimize the distance to the original, yielding a powerful adversarial example.

\paragraph{Foolbox}
Foolbox is a convenient Python~\cite{van1995python} toolbox for generating adversarial examples~\cite{rauber2017foolbox}. It supports a large collection of attacks, including all of the above.

\subsection{Quantifying Symmetry}

Since we found a surprising amount of symmetry in the confusion matrices in Section \ref{sec:results}, we wanted to somehow quantify exactly \textit{how} symmetric our matrices were. Upon a review of the literature we could not find any such metric, and so we decided to improvise our own.

Given the confusion matrix $A$ we first replace the diagonal values with zero, since these only represent failed attacks, to get $A_0$. We then split $A_0$ into its symmetric and anti-symmetric constituent matrices:

\begin{equation}
	A_{sym} = \frac{A_0 + A_0^T}{2}, \qquad
	A_{anti} = \frac{A_0 - A_0^T}{2}
\end{equation}
We then define our measure for symmetry $s$:

\begin{equation}
	s = \frac{ \|A_{sym}\|_1 - \|A_{anti}\|_1}{\|A_{sym}\|_1 + \|A_{anti}\|_1}
\end{equation}
For confusion matrices (where all values $a_{ij}$ are greater zero) $s$ can take values from $[0,1]$. $s = 1$ represents maximally symmetric matrices, and $s = 0$ a maximally anti-symmetric matrix.

Note that $s$ is well-defined for confusion matrices since any reordering of classes labels, that is permutations of the rows and columns of $A$, does not change $s$. Furthermore the 1-norm is not special, and could be replaced with any other $p$-norm, since it is well-defined for all of them.

Below are some examples to aid our esteemed reader in building an intuition.

\begin{equation*}
	s\bigg(\begin{bmatrix}
		0 & 1 \\
		0 & 0
	\end{bmatrix}\bigg) = 0, \qquad
%
	s\bigg(\begin{bmatrix}
		0 & 1 \\
		1 & 0
	\end{bmatrix}\bigg) = 1, \qquad
%
	s\bigg( \begin{bmatrix}
		0 & 1 \\
		0.5 & 0
	\end{bmatrix}\bigg) = 0.5
\end{equation*}
%\subsection{Neural networks (Necessary)}
%Is this section necessary? It seems that whoever is interested in our results, easily already knows this.
%\texttt{No, we can just mention neural networks in the introduction and specify our models in the experiments section}
%
%First introduced in \cite{lecun1999object}. The authors of
%\cite{krizhevsky2012imagenet} demonstrated the effectiveness of deep convolutional neural networks on ImageNet.
%
%\paragraph{ResNets}
%Paradigm shift in deep learning. In \cite{he2016deep} they developed Residual Networks to train very deep neural networks. We will probably use ResNet18. If we do, we probably also cite \cite{he2016identity} for the "pre-activation" optimization. This is just a better architecture obtained by having BatchNorm-ReLU-Weights blocks instead of Weights-BatchNorm-ReLU blocks.



\section{Experiments}
\label{sec:experiments}

In this Section we use the adversarial attacks introduced in Section~\ref{sec:Attacks} to compute adversarial examples on the MNIST~\cite{deng2012mnist}, Fashion-MNIST~\cite{xiao2017fashion} and CIFAR-10~\cite{krizhevsky2009learning} datasets. Each dataset is split into a training and test set and all adversarial attacks are computed and evaluated on the test set. Given a dataset and an attack we generate a pair of labels containing the original class and the predicted class for each adversarial example. We use $i$ to denote the $i$-th original class and $j$ to denote the $j$-th predicted class. Since all three datasets contain 10 classes each we obtain $10\times10$ confusion matrices
\[A=(a_{ij})_{\begin{subarray}{l} 1\le i\le 10 \\ 1\le j\le 10\end{subarray}},\]
where $a_{ij}\in\mathbb{N}$ corresponds to the total number of occurrences of each pair $(i,j)$.

\paragraph{Reproducability}All our code, Figures and the models we trained are available on GitHub \footnote{\url{https://github.com/MXSMCI/AdversarialLabelFlips}}.

\subsection{Experimental setup}
All experiments are conducted in Python 3.8.5~\cite{van1995python} using the PyTorch 1.8.1~\cite{pytorch} library on a Windows machine. Adversarial attacks are computed using Foolbox 3.3.1~\cite{rauber2017foolbox}. Each attack is instantiated using the Foolbox default parameters. All minimization attacks, i.e. \texttt{L0BrendelBethgeAttack}, \texttt{L1BrendelBethgeAttack}, \texttt{L2CarliniWagnerAttack} have their perturbation budget \texttt{epsilons} set to \texttt{None}. This prevents any early termination; a fixed number of iterations is used to compute the adversarial example with minimal perturbation size. For \texttt{LinfPGD} we set \texttt{epsilons} to one of the values in $[0.01, 0.02, 0.05, 0.1, 0.2, 0.5]$ to cover a wide range of perturbation budgets.

\subsection{Architectures}
We consider the MNIST Model and CIFAR Model, two architectures which are described in \cite{carlini2017towards}, Table 1 and 2. Both architectures consist of two blocks of Convolutional-Convolutional-MaxPooling \cite{nagi2011max} layers followed by three fully connected layers. All but the last layer use ReLU \cite{maas2013rectifier} as its activation function, whereas the final layer uses the softmax function.\\
Carlini, an author of \cite{carlini2017towards}, provides a reference implementation, which is available on GitHub \footnote{\url{https://github.com/carlini/nn_robust_attacks}}. We achieved similar results in terms of accuracy with our PyTorch reimplementation and can therefore confirm the validity of their results.  \\
For our experiments on the MNIST and Fashion-MNIST dataset we use the MNIST Model. For the CIFAR-10 dataset we use the CIFAR Model.

\section{Results}
\label{sec:results}
We present the confusion matrices for all adversarial attacks we considered. Figure \foreach \n in {CIFAR-10, FashionMNIST}{\ref{fig:\n-C-W},} and \ref{fig:MNIST-C-W} show the results for the Carlini-Wagner attack and Figure \foreach \n in {CIFAR-10, FashionMNIST}{\ref{fig:\n-PGD},} and \ref{fig:MNIST-PGD} show the results for all PGD attacks.
Since the results for both Brendel-Bethge attacks are similar to the Carlini-Wagner attack, we moved them to the appendix \ref{sec:BB}.  We add our symmetry score as well as the perturbation budget used $\epsilon$ to the Figure title, if applicable.

% Eine for loop in latex, wir sind jetzt bei den cool kids, Max!
\foreach \n in {CIFAR-10, FashionMNIST, MNIST}
{
	\subsection{\n}
		%\subsubsection{$L^2$-Carlini-Wagner}
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.5\textwidth]{../code/results/\n/figures/L2CarliniWagnerAttack.pdf}
			\caption{Confusion matrix for the $L^2$-Carlini-Wagner attack on \n. Attacks were computed using the Foolbox default parameters with no early termination.}
			\label{fig:\n-C-W}
		\end{figure}

	%\subsubsection{$L^\infty$-PGD}
	\begin{figure}[H]
		\begin{tabular}{cccc}
			\includegraphics[width=0.3\columnwidth]{../code/results/\n/figures/LinfPGD, epsilon=0.01.pdf} &
			\includegraphics[width=0.3\columnwidth]{../code/results/\n/figures/LinfPGD, epsilon=0.02.pdf} &
			\includegraphics[width=0.3\columnwidth]{../code/results/\n/figures/LinfPGD, epsilon=0.05.pdf} &
			\bigskip \\

			\includegraphics[width=0.3\columnwidth]{../code/results/\n/figures/LinfPGD, epsilon=0.1.pdf} &
			\includegraphics[width=0.3\columnwidth]{../code/results/\n/figures/LinfPGD, epsilon=0.2.pdf} &
			\includegraphics[width=0.3\columnwidth]{../code/results/\n/figures/LinfPGD, epsilon=0.5.pdf} &
		\end{tabular}

		\caption{Confusion matrices for the $L^\infty$-PGD attack on \n\ for varying maximal perturbation sizes per image pixel $\epsilon\in[0,1]$. Larger $\epsilon$ corresponds to stronger attacks. Attacks were computed using the Foolbox default parameters.}
		\label{fig:\n-PGD}
	\end{figure}

}

\newpage

\begin{figure}[t]
	\centering
	\includegraphics[width=\textwidth]{../code/results/barplot.pdf}
	\caption{Bar chart of the top 5 most common predictions of 1000 random uniform white noise images on CIFAR-10, MNIST and Fashion-MNIST respectively. On CIFAR-10 and MNIST only four and two different classes where predicted respectively.}
	\label{fig:barplot}
\end{figure}

\paragraph{Symmetry}
Probably the most notable feature of our results is the high degree of symmetry of the CIFAR-10 confusion matrices (Figures \ref{fig:CIFAR-10-C-W}, \ref{fig:CIFAR-10-B-B}, \ref{fig:CIFAR-10-PGD}). This means that the CNN is about as likely to mistake adversarial examples of class $i$ for $j$ as the other way around ($j$ for $i$). For example adversarial examples of cats are about as likely to be classified as "Dogs" as vice versa.

Upon taking a closer look at the classes that are likely to be mistaken for each other, our reader may notice a pattern. Across the board, the two most common causes of confusion are the pairs "Automobile"-"Truck", and "Dog"-"Cat". From a human perspective, this is an understandable mistake; these two categories are in fact similar in appearance. The other matrix entries follow a similar pattern: Animals are likely to be mistaken for other animals, and vehicles for other vehicles. Confusions of animals and vehicles are significantly rarer.

It therefore seems that the CNN captures some notion of similarity, that is quite close to what a human would intuit.
\vspace{12pt}


The MNIST, and FashionMNIST dataset do not appear to match this hypothesis too well.

In Figures \ref{fig:FashionMNIST-C-W} and \ref{fig:FashionMNIST-B-B} the classifier appears prone to confusing the pairs "Sandals"-"Sneakers" and "Coat"-"Pullover", but adversarial examples of Dresses are far more likely to be classified as "Trouser" than vice versa. The PGD-Matrices (Fig. \ref{fig:FashionMNIST-PGD}) have quite high symmetry scores, but nothing particularly captures the eye.

The MNIST confusion matrices yield comparatively low symmetry scores (Fig. \ref{fig:MNIST-C-W}, \ref{fig:MNIST-B-B}, \ref{fig:MNIST-PGD}). We propose that this is due to a overpowered CNN and resulting overfitting.

\paragraph{Catch-all Classes} In Figures \ref{fig:CIFAR-10-PGD} and \ref{fig:MNIST-PGD} one can observe that adversarial examples computed with large perturbation budgets $\epsilon$ are most often misclassified as "frog", and "8" for MNIST and CIFAR-10 respectively. For FashionMNIST \foreach \j in {FashionMNIST}{\ref{fig:\j-PGD},} there are multiple high probability classes. In order to shed light onto this phenomenon we generate and classify $10000$ white noise images sampled from a uniform distribution on the input domain. Figure \ref{fig:barplot} shows that these randomly generated images are also, most commonly, classified as "frog" and "8" respectively. More complex behaviour emerges for the Fashion-MNIST dataset. These results suggests that the neural networks in question have one or multiple default outputs for low probability images.
% with respect to the distribution of their respective input domains, 
This in turn affects adversarial examples computed with large perturbation budgets.


\section{Conclusion}
We explored confusion matrices generated by strong, well-studied untargeted attacks, and discovered two patterns.
\begin{itemize}
	\item Adversarial images with small perturbation sizes (e.g. small $\epsilon$ or computed with minimization attacks) can yield surprisingly symmetric confusion matrices. This symmetry means that class $i$ is confused with class $j$ about as often as vice versa. To us, this indicates that the classifier has learned that some classes are more closely related than others.
	This idea is corroborated by the fact that, in these cases, semantically similar classes are misclassified in a rather intuitive fashion; for instance confusing "Cat" with "Dog", and "Automobile" with "Truck".

	\item Attacks which are given a more generous $\epsilon$ tend to cluster into one or multiple specific class ("Frog" and "8"). We hypothesized that this is due the CNN using these classes as a catch-all for all images it has difficulty classifying. We sampled and predicted the class of white noise images to confirm this hypothesis.
\end{itemize}

\section{Contribution Statement}

This is joint work from Maximilian Samsinger and Matthias Dellago. Max contributed most of the code, which included training and evaluating the neural networks. Matthias wrote most parts of the this thesis. Both contributed equally to the presentations.
We thank Alexander Schlögl for the research idea and all members of the seminar for their valuable feedback, which lead to, among other revisions, the inclusion of Figure \ref{fig:barplot}.
\bibliographystyle{unsrt}
\bibliography{literature}

\appendix
\newpage
\section{Figures for Brendel-Bethge attacks}\label{sec:BB}

\foreach \n in {CIFAR-10, FashionMNIST, MNIST}
{	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../code/results/\n/figures/L0BrendelBethgeAttack.pdf}
			\caption{$L^0$}
			%\label{fig:y equals x}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=\textwidth]{../code/results/\n/figures/L1BrendelBethgeAttack.pdf}
			\caption{$L^1$}
			%\label{fig:}
		\end{subfigure}
		\caption{Confusion matrix for the Brendel-Bethge attack on \n. The left and right plot shows the $L^0$ and $L^1$ variant of the attack respectively. Both attacks were computed using the Foolbox default parameters with no early termination.}
		\label{fig:\n-B-B}
	\end{figure}
}

\end{document}
