\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{amsmath,amssymb,amsfonts,amsthm,mathtools}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{diagbox}
\usepackage{float, multirow}
\usepackage{tikz, pgfplots}
\usepackage{tikzsymbols}
\usetikzlibrary{spy}
\usepackage{subcaption}
\usepgfplotslibrary{groupplots}
\pgfplotsset{compat=newest}
\usepackage{blindtext}

\newcommand{\defneq}{\mathrel{\mathop:}=}
\newcommand{\eqdefn}{=\mathrel{\mathop:}}
\begin{document}

\begin{titlepage}
	\noindent\makebox[\textwidth][l]{\includegraphics{universitaet-innsbruck-logo-cmyk-farbe.pdf}}
	\vspace{3cm}
	\begin{center}
		{\Large Seminar thesis}
		\vspace{50pt}\\
		\textbf{\Huge Adversarial Label Flips (sexier title?)}
		\vspace{40pt}\\
		\textbf{\Large Matthias Dellago \& Maximilian Samsinger}\vspace{20pt}\\
		{\large\today}
		\vspace{120pt}
	\end{center}
\end{titlepage}
	


\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
	
\begin{abstract}
	%sell it to sb looking for something good to read.
	%only 4 sentences or so.
	
	Given a neural network (NN) trained to classify and a untargeted evasion attack \cite{}, in what class does the adversarial example fall?
	In the following, we will answer this question by evaluating some state of the art attacks, on a simple NN trained on industry standard datasets \cite{mnist, fashion,mnist, cifar10}.
	We discover a intuitive symmetry in the classification of adversarial examples.
\end{abstract}

\section{Introduction}

Adversarial examples exist.
Targeted and untargeted.
What does that look like?
We use confusion matrices.
example.
how did we do it? -> NN + foolbox + mnist cifar (section methods)
what did we get out of it? -> symmetric matrices and interesting insight about noise. (section results)


In 2013 Szegedy et al. demonstrated that attacking deep neural networks (NN) are susceptible to attacks \cite{Szegedy13}. These adversarial examples consist of a small perturbation applied to a otherwise innocuous input, engineered to cause the NN to misbehave.

In our case, the inputs will be images and the attack will be small changes to said image, designed to cause the classifier NN to misclassify the target.

These attacks on classifiers come in two different variations: targeted and untargeted. 
In targeted attacks\cite{} the attacker aims to have the adversarial example identified as a specific class by the NN. (Say, misclassify a dog as a cat.)
An untargeted attack meanwhile, only tries to evade correct classification. (Make this dog appear as anything, apart from a dog.) 

Now, when considering untargeted attacks, the question as what the adversarial image $actually$ is then classified, naturally arises. This is what we will experimentally answer in this paper.

We will present our results in terms of confusion matrices. In figure !! you can see an example. In larger matrices numbers become more difficult to process, so we will display our results in heatmap-style images (fig. !!). (Section results)

In our experiments we used the foolbox framework \cite{}, and simple NNs trained on the MNIST, FashionMNIST, and CIFAR-10 datasets \cite{}. We applied three state of the art attacks: Projected Gradient Decent (PDG)\cite{}, Carlini-Wagner \cite{} and ...-Bethge\cite{}. 

We show that, for the CIFAR-10 dataset the confusion matrices are surprisingly symmetric, and intuitively similar classes are often confused with each other. Furthermore we observe that for attacks which can choose large pertubations, there exist certain attractor classes, which most of the adversarial images are classified as. (Section Results.)

%\\
%
%what is our contribution. (we show that these confusion matrix are surprisingly nonrandom)
%\\
%use forward references i.e. "we elaborate on this in sectrion 4".\\
%Give an example for a confusion matrix straight away.



\section{Background and related work}

\paragraph{Existence of adversarial examples}
Since being first demonstrated \cite{Szegedy13} a large body of literature has flourished around adversarial examples. Blablabla...

\paragraph{Notions of similarity}
As mentioned previously we want to fool the NN with a small perturbation, that is to say a image which is very similar. But what do we precisely mean by "small pertubation" and "similar"? We could argue that an image with only one pixel changed is very similar, but also that an image in which each pixel was altered only very slightly is similar.

These different intuitive notions of distance and similarity are captured by norms.

The distance of two images ($x$,$y$) (i.e. vectors) is described by a metric $D$. This metric can be defined via different norms, most commonly the $L^\infty$-, $L^1$- and $L^2$-norms.( The $L^0$-"norm" is also frequently used, though not a norm in the strict sense.) The distance between these the two images, is then defined as the $L^p$-norm of their difference, for a given $p$:

\begin{equation}
	D(x,y) \defneq \norm{y-x}_p % Here the := is fixed and i created a command for the norm. Hope that helps!
\end{equation}
\begin{equation}
	D(x,y) := \| y - x \|_p % Previous version for reference
\end{equation}


\textbf{... Hmmm probably not that important, maybe if there is time and space after everything else in finished.}

\subsection{Attacks}\label{sec:Attacks}

\paragraph{Fast gradient sign method}
Goodfellow et al. famously developed the fast gradient sign method (FGSM) \cite{goodfellow2014explaining}, making attacks fast and easy. Its key insight was that the backpropagation commonly used to update the weights and biases can be applied all the way back to the input data itself to yields the gradient of the cost function. They then apply gradient decent to find an adversarial example. Since they optimise for the $L^\infty$-norm, all entries of the perturbance are scaled to the same magnitude.

\paragraph{Projected gradient descent}
Projected gradient descent (PGD) was first shown in \cite{madry2017towards}. Conceptually it is very similar to iterating FGSM until converging in a local misclassification optimum. The "projected" part of the name, derives from the fact that upon leaving a ball of radius $\epsilon$ instead of continuing iteration, they project back onto said ball. From iterated FGSM resumes.
This attack leads to formidable results, especially using the $L^\infty$-norm.


% Their experiments suggest that these attacks converge, i.e. they find a local maxima. This may require some restarts.

\paragraph{Carlini-Wagner attack}

Carlini and Wagner \cite{carlini2017towards} invented a different style of attack, where the cost function of the classifier and the distance of the adversarial example are wrapped into one function. They can then simultaneously optimise for both using the Adam stochastic optimiser \cite{kingma2017adam}.

\paragraph{Brendel-Bethge attack}

Brendel and Bethe invented a quite different method, which they aptly named Brendel \& Bethge attacks \cite{brendel2019accurate}. 



%\subsection{Neural networks (Necessary)}
%Is this section necessary? It seems that whoever is interested in our results, easily already knows this.
%\texttt{No, we can just mention neural networks in the introduction and specify our models in the experiments section}
%
%First introduced in \cite{lecun1999object}. The authors of
%\cite{krizhevsky2012imagenet} demonstrated the effectiveness of deep convolutional neural networks on ImageNet.
%
%\paragraph{ResNets}
%Paradigm shift in deep learning. In \cite{he2016deep} they developed Residual Networks to train very deep neural networks. We will probably use ResNet18. If we do, we probably also cite \cite{he2016identity} for the "pre-activation" optimization. This is just a better architecture obtained by having BatchNorm-ReLU-Weights blocks instead of Weights-BatchNorm-ReLU blocks.


\section{Experiments}\label{experiments}
In this Section we use the adversarial attacks introduced in Section~\ref{sec:Attacks} to compute adversarial examples on the MNIST~\cite{deng2012mnist}, Fashion-MNIST~\cite{deng2012mnist} and CIFAR-10~\cite{krizhevsky2009learning} datasets. Each dataset is split into a training and test set and all adversarial attacks are computed and evaluated on the test set. Given a dataset and an attack we generate a pair of labels containing the original class and the predicted class for each adversarial example. We use $i$ to denote the $i$-th original class and $j$ to denote the $j$-th predicted class. Since all three datasets contain 10 classes each we obtain $10\times10$ confusion matrices
\[A=(a_{ij})_{\begin{subarray}{l} 1\le i\le 10 \\ 1\le j\le 10\end{subarray}},\]
where $a_{ij}\in\mathbb{N}$ corresponds to the total number of occurrences of each pair $(i,j)$.


\subsection{Experimental setup}
All experiments are conducted in Python 3.8.5~\cite{van1995python} using the PyTorch 1.8.1~\cite{pytorch} library on a Windows machine. Adversarial attacks are computed using FoolBox 3.3.1~\cite{rauber2017foolbox}. Each attack is instantiated using the FoolBox default parameters. All minimization attacks, i.e. \texttt{L0BrendelBethgeAttack}, \texttt{L1BrendelBethgeAttack}, \texttt{L2CarliniWagnerAttack} have their perturbation budget \texttt{epsilons} set to \texttt{None}. This prevents any early termination; a fixed number of iterations is used to compute the adversarial example with minimal perturbation size. For \texttt{LinfPGD} we set \texttt{epsilons} to one of the values in $[0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1]$ to cover a wide range of perturbation budgets.\\

\noindent Reference to figures.\\
Reference to our github.

\subsection{Architectures}
\url{https://arxiv.org/pdf/1608.04644.pdf}


\section{Discussion}
Symmetry of matrices -> maybe find a way to quantify symmetry?\\
-> NN can recognise "similarity"\\
\\

Attractor classes -> manage with an extra "noise"-class or so?\\


In Figures X, Y and Z one can observe that adversarial examples computed with large perturbation budgets $\epsilon$ are misclassified as "8", "TODO" and "frog" for MNIST, Fashion-MNIST and CIFAR-10 respectively. In order to shed light onto this phenomenon we generate and classify $10000$ white noise images sampled from a uniform distribution on the input domain. Figure A shows that these randomly generated images are also, most commonly, classified as "8", "TODO" and "frog" respectively. This result suggests that the neural networks in question have a default output for low probability images with respect to distribution of the input domain, which in turn affects adversarial examples computed with large perturbation budgets.




\section{Conclusion}
What was the main idea.

\section{Contribution Statement}

This is joint work from Maximilian Samsinger and Matthias Dellago. Max wrote the code...
We thank Alexander Schl√∂gl for the research idea.
\bibliographystyle{unsrt}
\bibliography{literature}

\end{document}
